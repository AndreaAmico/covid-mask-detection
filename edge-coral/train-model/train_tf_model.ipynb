{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 160 # All images will be resized to 160x160\n",
    "\n",
    "def format_example(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/127.5) - 1\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "base_model.trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "# prediction_layer = tf.keras.layers.Dense(1)\n",
    "prediction_layer = tf.keras.layers.Dense(2, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 649 images belonging to 2 classes.\n",
      "Found 142 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"facemask-dataset/dest_folder/train\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(160, 160))\n",
    "\n",
    "VALIDATION_DIR = \"facemask-dataset/dest_folder/val\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(160, 160))\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "64/65 [============================>.] - ETA: 2s - loss: 0.6924 - accuracy: 0.6495WARNING:tensorflow:From C:\\Users\\aamico\\AppData\\Local\\Continuum\\anaconda3\\envs\\documented\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "65/65 [==============================] - 195s 3s/step - loss: 0.6919 - accuracy: 0.6487 - val_loss: 0.6241 - val_accuracy: 0.8592\n",
      "Epoch 2/15\n",
      "64/65 [============================>.] - ETA: 1s - loss: 0.6571 - accuracy: 0.7637INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "65/65 [==============================] - 145s 2s/step - loss: 0.6566 - accuracy: 0.7658 - val_loss: 0.6068 - val_accuracy: 0.8662\n",
      "Epoch 3/15\n",
      "64/65 [============================>.] - ETA: 1s - loss: 0.6228 - accuracy: 0.8451INFO:tensorflow:Assets written to: model-003.model\\assets\n",
      "65/65 [==============================] - 152s 2s/step - loss: 0.6229 - accuracy: 0.8444 - val_loss: 0.5879 - val_accuracy: 0.9296\n",
      "Epoch 4/15\n",
      "64/65 [============================>.] - ETA: 1s - loss: 0.6075 - accuracy: 0.8701INFO:tensorflow:Assets written to: model-004.model\\assets\n",
      "65/65 [==============================] - 138s 2s/step - loss: 0.6071 - accuracy: 0.8706 - val_loss: 0.5662 - val_accuracy: 0.9648\n",
      "Epoch 5/15\n",
      "64/65 [============================>.] - ETA: 1s - loss: 0.5895 - accuracy: 0.9014INFO:tensorflow:Assets written to: model-005.model\\assets\n",
      "65/65 [==============================] - 142s 2s/step - loss: 0.5896 - accuracy: 0.9014 - val_loss: 0.5569 - val_accuracy: 0.9789\n",
      "Epoch 6/15\n",
      "64/65 [============================>.] - ETA: 2s - loss: 0.5832 - accuracy: 0.9249INFO:tensorflow:Assets written to: model-006.model\\assets\n",
      "65/65 [==============================] - 186s 3s/step - loss: 0.5834 - accuracy: 0.9230 - val_loss: 0.5523 - val_accuracy: 0.9718\n",
      "Epoch 7/15\n",
      "64/65 [============================>.] - ETA: 1s - loss: 0.5782 - accuracy: 0.9045INFO:tensorflow:Assets written to: model-007.model\\assets\n",
      "65/65 [==============================] - 166s 3s/step - loss: 0.5778 - accuracy: 0.9060 - val_loss: 0.5451 - val_accuracy: 0.9718\n",
      "Epoch 8/15\n",
      "65/65 [==============================] - 121s 2s/step - loss: 0.5696 - accuracy: 0.9214 - val_loss: 0.5460 - val_accuracy: 0.9789\n",
      "Epoch 9/15\n",
      "64/65 [============================>.] - ETA: 1s - loss: 0.5699 - accuracy: 0.9155INFO:tensorflow:Assets written to: model-009.model\\assets\n",
      "65/65 [==============================] - 144s 2s/step - loss: 0.5693 - accuracy: 0.9168 - val_loss: 0.5408 - val_accuracy: 0.9718\n",
      "Epoch 10/15\n",
      "64/65 [============================>.] - ETA: 1s - loss: 0.5629 - accuracy: 0.9327INFO:tensorflow:Assets written to: model-010.model\\assets\n",
      "65/65 [==============================] - 145s 2s/step - loss: 0.5624 - accuracy: 0.9337 - val_loss: 0.5381 - val_accuracy: 0.9718\n",
      "Epoch 11/15\n",
      "64/65 [============================>.] - ETA: 1s - loss: 0.5673 - accuracy: 0.9218INFO:tensorflow:Assets written to: model-011.model\\assets\n",
      "65/65 [==============================] - 145s 2s/step - loss: 0.5668 - accuracy: 0.9230 - val_loss: 0.5354 - val_accuracy: 0.9789\n",
      "Epoch 12/15\n",
      "65/65 [==============================] - 114s 2s/step - loss: 0.5575 - accuracy: 0.9445 - val_loss: 0.5368 - val_accuracy: 0.9718\n",
      "Epoch 13/15\n",
      "65/65 [==============================] - 116s 2s/step - loss: 0.5528 - accuracy: 0.9353 - val_loss: 0.5469 - val_accuracy: 0.9718\n",
      "Epoch 14/15\n",
      "64/65 [============================>.] - ETA: 1s - loss: 0.5563 - accuracy: 0.9296INFO:tensorflow:Assets written to: model-014.model\\assets\n",
      "65/65 [==============================] - 150s 2s/step - loss: 0.5558 - accuracy: 0.9307 - val_loss: 0.5293 - val_accuracy: 0.9789\n",
      "Epoch 15/15\n",
      "64/65 [============================>.] - ETA: 1s - loss: 0.5587 - accuracy: 0.9124INFO:tensorflow:Assets written to: model-015.model\\assets\n",
      "65/65 [==============================] - 152s 2s/step - loss: 0.5586 - accuracy: 0.9122 - val_loss: 0.5274 - val_accuracy: 0.9789\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=15,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model-015.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From tf to tf-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = 'model-015.model'\n",
    "\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
